"""Benchmark the latency of awq layers on VLLM."""

from vllm import _custom_ops as ops
import torch

if __name__ == "__main__":
    qweight = torch.tensor([[ 1502046311,  1469741427, -2006554745, -2005378696,
          2107086155, -1922665641],
        [ 1573025058,   902464929,  1258845848, -1717020234,
           946432358,  1003189687],
        [ 1216817254,  1463585543, -1825286118, -954693018,
         -1453517939,  -822240646]]).to(dtype=torch.int32).cuda()
    scales = torch.tensor([[0.1163, 0.3103, 0.2102, 0.0256, 0.0274, 0.0223],
        [0.1744, 0.4380, 0.3767, 0.0296, 0.0246, 0.0153],
        [0.1703, 0.3245, 0.1812, 0.0226, 0.0177, 0.0213]]).to(dtype=torch.float16).cuda()
    qzeros = torch.tensor([[ 1502046327,  1738176869, -2023331960, -1737913992,
         -1719109784, -1702270872],
        [ 1752598391,  2025289620,  1989650566, 1740015752,
          1969784439,  2004313956],
        [-1415018873, -1520990822, -1466526088, -1433831289,
          1735829640, -1752856713]]).to(dtype=torch.int32).cuda()
    pack_factor = 8
    reshaped_x = torch.tensor([[ 1.2741e-03,  7.7667e-03, -6.2370e-03, 2.3041e-03,
         -1.6088e-03, -1.1861e-05],
        [ 1.2741e-03,  7.7667e-03, -6.2370e-03,  2.3041e-03,
         -1.6088e-03, -1.1861e-05],
        [ 1.2741e-03,  7.7667e-03, -6.2370e-03, 2.3041e-03,
         -1.6088e-03, -1.1861e-05]]).to(dtype=torch.float16).cuda()
    
    out = out = ops.awq_gemm(reshaped_x, qweight, scales, qzeros, pack_factor)